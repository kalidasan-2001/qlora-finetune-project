{"messages":[{"role":"system","content":"You are a helpful, concise assistant."},{"role":"user","content":"What is QLoRA?"},{"role":"assistant","content":"QLoRA fine-tunes a 4-bit quantized base model using small LoRA adapters to save memory."}]}
{"messages":[{"role":"system","content":"You are a helpful, concise assistant."},{"role":"user","content":"Give 3 quick Python learning tips."},{"role":"assistant","content":"Practice daily, read standard library docs, and build small projects."}]}
{"messages":[{"role":"system","content":"You are a helpful, concise assistant."},{"role":"user","content":"Explain LoRA briefly."},{"role":"assistant","content":"LoRA trains low-rank adapter matrices while keeping the original model weights frozen."}]}
{"messages":[{"role":"system","content":"You are a helpful, concise assistant."},{"role":"user","content":"Summarize QLoRA in 2 sentences."},{"role":"assistant","content":"QLoRA loads a model in low-bit precision and trains lightweight adapters instead of full weights. This keeps quality high while reducing GPU memory usage."}]}
{"messages":[{"role":"system","content":"You are a helpful, concise assistant."},{"role":"user","content":"Example: what does Docker do?"},{"role":"assistant","content":"Docker packages an app and its dependencies into a container image so it runs consistently anywhere."}]}
{"messages":[{"role":"system","content":"You are a helpful, concise assistant."},{"role":"user","content":"Why combine quantization with LoRA?"},{"role":"assistant","content":"Quantization shrinks memory; LoRA keeps training lightweight by updating only small adapter parameters."}]}
